# -*- coding: utf-8 -*-
"""
grad_runner.py

Refactored for Flask API usage with safe initialization.
"""

import os
from dotenv import load_dotenv
import json
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
import requests

# ----------------------------
# LOAD ENVIRONMENT VARIABLES
# ----------------------------

load_dotenv()

CORPUS_PATH = os.environ.get(
    "NHS_CORPUS_PATH",
    default=os.path.join(os.path.dirname(__file__), "nhs_conditions.json")
)

# ----------------------------
# GLOBALS (lazy initialized)
# ----------------------------

embedder = None
index = None
nhs_docs = None

# ----------------------------
# INITIALIZER FUNCTION
# ----------------------------

def initialize_model():
    """
    Loads embeddings and builds the FAISS index if not already loaded.
    """
    global embedder, index, nhs_docs

    if embedder is not None:
        # Already initialized
        return

    # Load NHS corpus
    print("⚙️ Loading NHS corpus from:", CORPUS_PATH)
    with open(CORPUS_PATH, "r", encoding="utf-8") as f:
        nhs_docs_raw = json.load(f)

    # Deduplicate documents
    unique_docs = []
    seen_texts = set()
    for doc in nhs_docs_raw:
        text = doc["text"].strip()
        if text not in seen_texts:
            seen_texts.add(text)
            unique_docs.append(doc)

    nhs_docs = unique_docs
    doc_texts = [doc["text"] for doc in nhs_docs]

    # Load embedding model
    print("⚙️ Loading SentenceTransformer model...")
    embedder = SentenceTransformer("multi-qa-MiniLM-L6-cos-v1")

    # Embed all documents
    print("⚙️ Generating embeddings for corpus...")
    doc_embeddings = embedder.encode(
        doc_texts,
        normalize_embeddings=True,
        show_progress_bar=True
    )

    # Build FAISS index
    dimension = doc_embeddings.shape[1]
    index_local = faiss.IndexFlatIP(dimension)
    index_local.add(doc_embeddings)

    index = index_local
    print(f"✅ FAISS index built with {index.ntotal} vectors.")

# ----------------------------
# PROMPT BUILDER
# ----------------------------

def build_combined_prompt(context_docs, user_query):
    context_text = ""

    for doc in context_docs:
        snippet = doc["text"][:800]  # Limit to first 800 characters
        context_text += f"[NHS] {doc['title']}. {snippet}\n\n"

    prompt = f"""
You are MediBot, a virtual medical assistant.

Use the information below if it answers the user's question.
If it doesn't fully answer the question, also use your own medical knowledge.
Speak in clear, simple language.

Do not discuss surgery, invasive interventions, or complex specialist treatments unless explicitly required by the context or asked by the user. Prefer conservative recommendations like self-care, physical therapy, or seeing a primary care doctor.

Always end your answer with: "This information is not a replacement for a doctor’s advice. Please consult a healthcare professional for your specific needs."

In case of non-medical prompts, don't respond to the prompt context and state that you're a medical chatbot and you're here for any medical advice only, don't advise anything else.

Context:
{context_text}

User question:
{user_query}
"""
    return prompt.strip()

# ----------------------------
# PREDICT FUNCTION
# ----------------------------

def predict(user_query):
    """
    Takes a user query string, performs retrieval and sends the prompt
    to the LlamaMedicine model via Ollama API. Returns the model's response.
    """

    initialize_model()   # Make sure model is loaded

    # Embed user query
    query_emb = embedder.encode([user_query], normalize_embeddings=True)[0]

    # Search top 1 matching document
    k = 1
    D, I = index.search(query_emb.reshape(1, -1), k=k)

    filtered_docs = []
    for idx, score in zip(I[0], D[0]):
        doc = nhs_docs[idx]
        filtered_docs.append(doc)
        print(f"✅ Match found: {doc['title']} (score={score:.3f})")

    # Build the combined prompt
    prompt = build_combined_prompt(filtered_docs, user_query)

    # Send request to Ollama API
    payload = {
        "model": "elixpo/llamamedicine",
        "prompt": prompt,
        "stream": False,
        "options": {
            "num_predict": 256
        }
    }

    response = requests.post(
        "http://localhost:11434/api/generate",
        json=payload,
        timeout=300
    )
    response.raise_for_status()
    result = response.json()

    final_answer = result.get("response", "").strip()

    print("\n=== LlamaMedicine's Answer ===\n")
    print(final_answer)

    return final_answer

